{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c41ea6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from typing import Dict, List, Any, Optional\n",
    "import json\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from data_loading import build_panel_dataset\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e1f9965",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:data_loading:Starting panel dataset construction...\n",
      "INFO:data_loading:Found 110 CSV files to merge\n",
      "INFO:data_loading:Successfully loaded shrt20230315.csv with 19,946 rows\n",
      "INFO:data_loading:Successfully loaded shrt20221230.csv with 20,186 rows\n",
      "INFO:data_loading:Successfully loaded shrt20220729.csv with 20,601 rows\n",
      "INFO:data_loading:Successfully loaded shrt20220715.csv with 20,732 rows\n",
      "INFO:data_loading:Successfully loaded shrt20210615.csv with 20,251 rows\n",
      "INFO:data_loading:Successfully loaded shrt20240430.csv with 19,468 rows\n",
      "INFO:data_loading:Successfully loaded shrt20221031.csv with 20,696 rows\n",
      "INFO:data_loading:Successfully loaded shrt20240815.csv with 19,530 rows\n",
      "INFO:data_loading:Successfully loaded shrt20220930.csv with 20,742 rows\n",
      "INFO:data_loading:Successfully loaded shrt20210415.csv with 19,816 rows\n",
      "INFO:data_loading:Successfully loaded shrt20241115.csv with 19,729 rows\n",
      "INFO:data_loading:Successfully loaded shrt20241129.csv with 19,946 rows\n",
      "INFO:data_loading:Successfully loaded shrt20210831.csv with 20,123 rows\n",
      "INFO:data_loading:Successfully loaded shrt20211130.csv with 20,409 rows\n",
      "INFO:data_loading:Successfully loaded shrt20210212.csv with 19,356 rows\n",
      "INFO:data_loading:Successfully loaded shrt20250430.csv with 20,325 rows\n",
      "INFO:data_loading:Successfully loaded shrt20231031.csv with 19,303 rows\n",
      "INFO:data_loading:Successfully loaded shrt20220114.csv with 20,503 rows\n",
      "INFO:data_loading:Successfully loaded shrt20230515.csv with 19,774 rows\n",
      "INFO:data_loading:Successfully loaded shrt20220315.csv with 20,747 rows\n",
      "INFO:data_loading:Successfully loaded shrt20220513.csv with 20,709 rows\n",
      "INFO:data_loading:Successfully loaded shrt20230714.csv with 19,322 rows\n",
      "INFO:data_loading:Successfully loaded shrt20230113.csv with 20,125 rows\n",
      "INFO:data_loading:Successfully loaded shrt20250630.csv with 20,516 rows\n",
      "INFO:data_loading:Successfully loaded shrt20250131.csv with 19,999 rows\n",
      "INFO:data_loading:Successfully loaded shrt20220429.csv with 20,739 rows\n",
      "INFO:data_loading:Successfully loaded shrt20210930.csv with 20,111 rows\n",
      "INFO:data_loading:Successfully loaded shrt20221130.csv with 20,547 rows\n",
      "INFO:data_loading:Successfully loaded shrt20240531.csv with 19,491 rows\n",
      "INFO:data_loading:Successfully loaded shrt20220831.csv with 20,764 rows\n",
      "INFO:data_loading:Successfully loaded shrt20220414.csv with 20,797 rows\n",
      "INFO:data_loading:Successfully loaded shrt20210528.csv with 20,132 rows\n",
      "INFO:data_loading:Successfully loaded shrt20241015.csv with 19,755 rows\n",
      "INFO:data_loading:Successfully loaded shrt20210514.csv with 20,006 rows\n",
      "INFO:data_loading:Successfully loaded shrt20240731.csv with 19,527 rows\n",
      "INFO:data_loading:Successfully loaded shrt20230215.csv with 20,099 rows\n",
      "INFO:data_loading:Successfully loaded shrt20230228.csv with 19,992 rows\n",
      "INFO:data_loading:Successfully loaded shrt20211231.csv with 20,317 rows\n",
      "INFO:data_loading:Successfully loaded shrt20220615.csv with 20,752 rows\n",
      "INFO:data_loading:Successfully loaded shrt20250331.csv with 20,270 rows\n",
      "INFO:data_loading:Successfully loaded shrt20210715.csv with 20,164 rows\n",
      "INFO:data_loading:Successfully loaded shrt20240913.csv with 19,566 rows\n",
      "INFO:data_loading:Successfully loaded shrt20250731.csv with 20,595 rows\n",
      "INFO:data_loading:Successfully loaded shrt20210315.csv with 19,580 rows\n",
      "INFO:data_loading:Successfully loaded shrt20220215.csv with 20,648 rows\n",
      "INFO:data_loading:Successfully loaded shrt20220228.csv with 20,696 rows\n",
      "INFO:data_loading:Successfully loaded shrt20230615.csv with 19,611 rows\n",
      "INFO:data_loading:Successfully loaded shrt20241213.csv with 19,883 rows\n",
      "INFO:data_loading:Successfully loaded shrt20240131.csv with 19,271 rows\n",
      "INFO:data_loading:Successfully loaded shrt20250530.csv with 20,362 rows\n",
      "INFO:data_loading:Successfully loaded shrt20231130.csv with 19,205 rows\n",
      "INFO:data_loading:Successfully loaded shrt20210115.csv with 18,646 rows\n",
      "INFO:data_loading:Successfully loaded shrt20210129.csv with 18,948 rows\n",
      "INFO:data_loading:Successfully loaded shrt20230414.csv with 19,922 rows\n",
      "INFO:data_loading:Successfully loaded shrt20230428.csv with 19,854 rows\n",
      "INFO:data_loading:Successfully loaded shrt20230831.csv with 19,336 rows\n",
      "INFO:data_loading:Successfully loaded shrt20231115.csv with 19,222 rows\n",
      "INFO:data_loading:Successfully loaded shrt20250515.csv with 20,354 rows\n",
      "INFO:data_loading:Successfully loaded shrt20230815.csv with 19,367 rows\n",
      "INFO:data_loading:Successfully loaded shrt20240328.csv with 19,371 rows\n",
      "INFO:data_loading:Successfully loaded shrt20230630.csv with 19,465 rows\n",
      "INFO:data_loading:Successfully loaded shrt20250715.csv with 20,611 rows\n",
      "INFO:data_loading:Successfully loaded shrt20210331.csv with 19,677 rows\n",
      "INFO:data_loading:Successfully loaded shrt20240315.csv with 19,387 rows\n",
      "INFO:data_loading:Successfully loaded shrt20241231.csv with 19,763 rows\n",
      "INFO:data_loading:Successfully loaded shrt20210730.csv with 20,156 rows\n",
      "INFO:data_loading:Successfully loaded shrt20250314.csv with 20,194 rows\n",
      "INFO:data_loading:Successfully loaded shrt20220630.csv with 20,724 rows\n",
      "INFO:data_loading:Successfully loaded shrt20240715.csv with 19,573 rows\n",
      "INFO:data_loading:Successfully loaded shrt20211215.csv with 20,312 rows\n",
      "INFO:data_loading:Successfully loaded shrt20240112.csv with 19,223 rows\n",
      "INFO:data_loading:Successfully loaded shrt20250115.csv with 19,921 rows\n",
      "INFO:data_loading:Successfully loaded shrt20211015.csv with 20,141 rows\n",
      "INFO:data_loading:Successfully loaded shrt20211029.csv with 20,188 rows\n",
      "INFO:data_loading:Successfully loaded shrt20221115.csv with 20,787 rows\n",
      "INFO:data_loading:Successfully loaded shrt20240515.csv with 19,471 rows\n",
      "INFO:data_loading:Successfully loaded shrt20240930.csv with 19,692 rows\n",
      "INFO:data_loading:Successfully loaded shrt20210915.csv with 20,060 rows\n",
      "INFO:data_loading:Successfully loaded shrt20220815.csv with 20,781 rows\n",
      "INFO:data_loading:Successfully loaded shrt20241031.csv with 19,659 rows\n",
      "INFO:data_loading:Successfully loaded shrt20220331.csv with 20,848 rows\n",
      "INFO:data_loading:Successfully loaded shrt20230731.csv with 19,442 rows\n",
      "INFO:data_loading:Successfully loaded shrt20240229.csv with 19,276 rows\n",
      "INFO:data_loading:Successfully loaded shrt20240215.csv with 19,323 rows\n",
      "INFO:data_loading:Successfully loaded shrt20210813.csv with 20,109 rows\n",
      "INFO:data_loading:Successfully loaded shrt20231215.csv with 19,268 rows\n",
      "INFO:data_loading:Successfully loaded shrt20231229.csv with 19,224 rows\n",
      "INFO:data_loading:Successfully loaded shrt20210226.csv with 19,495 rows\n",
      "INFO:data_loading:Successfully loaded shrt20230531.csv with 19,707 rows\n",
      "INFO:data_loading:Successfully loaded shrt20230915.csv with 19,297 rows\n",
      "INFO:data_loading:Successfully loaded shrt20230929.csv with 19,209 rows\n",
      "INFO:data_loading:Successfully loaded shrt20220131.csv with 20,554 rows\n",
      "INFO:data_loading:Successfully loaded shrt20250415.csv with 20,324 rows\n",
      "INFO:data_loading:Successfully loaded shrt20211115.csv with 20,385 rows\n",
      "INFO:data_loading:Successfully loaded shrt20250613.csv with 20,462 rows\n",
      "INFO:data_loading:Successfully loaded shrt20220531.csv with 20,832 rows\n",
      "INFO:data_loading:Successfully loaded shrt20210430.csv with 19,913 rows\n",
      "INFO:data_loading:Successfully loaded shrt20220915.csv with 20,768 rows\n",
      "INFO:data_loading:Successfully loaded shrt20240830.csv with 19,514 rows\n",
      "INFO:data_loading:Successfully loaded shrt20230131.csv with 20,189 rows\n",
      "INFO:data_loading:Successfully loaded shrt20221014.csv with 20,754 rows\n",
      "INFO:data_loading:Successfully loaded shrt20240415.csv with 19,464 rows\n",
      "INFO:data_loading:Successfully loaded shrt20231013.csv with 19,369 rows\n",
      "INFO:data_loading:Successfully loaded shrt20230331.csv with 19,959 rows\n",
      "INFO:data_loading:Successfully loaded shrt20250228.csv with 20,101 rows\n",
      "INFO:data_loading:Successfully loaded shrt20250214.csv with 20,108 rows\n",
      "INFO:data_loading:Successfully loaded shrt20210630.csv with 20,134 rows\n",
      "INFO:data_loading:Successfully loaded shrt20221215.csv with 20,389 rows\n",
      "INFO:data_loading:Successfully loaded shrt20240614.csv with 19,450 rows\n",
      "INFO:data_loading:Successfully loaded shrt20240628.csv with 19,427 rows\n",
      "INFO:data_loading:Successfully merged 110 files into 2,197,833 rows\n",
      "INFO:data_loading:Successfully loaded CRSP Market Data 2.csv with 9,336,853 rows\n",
      "INFO:data_loading:CRSP data cleaned: 9,318,438 rows after filtering\n",
      "INFO:data_loading:Merged short interest: 9,318,438 rows\n",
      "INFO:data_loading:Successfully loaded IBES Recommendations.csv with 76,263 rows\n",
      "INFO:data_loading:Recommendations aggregated: 71,172 consensus records\n",
      "INFO:data_loading:Merged recommendations: 1,942,322 of 9,318,438 rows matched within 90 days 00:00:00\n",
      "INFO:data_loading:Successfully loaded Compustat Fundamentals.csv with 221,592 rows\n",
      "INFO:data_loading:Fundamentals data prepared: 221,592 rows\n",
      "INFO:data_loading:Merged fundamentals: 0 of 9,318,438 rows matched within 180 days 00:00:00\n",
      "INFO:data_loading:Panel dataset construction complete: 9,318,438 rows, 63 columns\n"
     ]
    }
   ],
   "source": [
    "# Define paths relative to the notebook location\n",
    "data_folder = Path('..') / '..' / 'Data' / 'Short Interest Data'\n",
    "crsp_file = Path('..') / '..' / 'Data' / 'CRSP Market Data 2.csv'\n",
    "ibes_file = Path('..') / '..' / 'Data' / 'IBES Recommendations.csv'\n",
    "compustat_file = Path('..') / '..' / 'Data' / 'Compustat Fundamentals.csv'\n",
    "\n",
    "# Load with your paths\n",
    "panel_df = build_panel_dataset(\n",
    "    data_folder=data_folder,\n",
    "    crsp_file=crsp_file,\n",
    "    ibes_file=ibes_file,\n",
    "    compustat_file=compustat_file\n",
    ")\n",
    "panel_df = panel_df.set_index(['PERMNO', 'date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a47c1e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.int64(84588), Timestamp('2021-01-04 00:00:00'))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b49332e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import json\n",
    "\n",
    "class FeatureEngineeringAgentLLM:\n",
    "    def __init__(self, model_name: str):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "    def _build_prompt(self, data_analysis: Dict, target_column: str) -> str:\n",
    "        prompt = f\"\"\"\n",
    "You are a feature engineering expert for time series forecasting.\n",
    "\n",
    "Data Analysis:\n",
    "- Shape: {data_analysis['shape']}\n",
    "- Target Column: {target_column}\n",
    "- Numeric Columns: {data_analysis['numeric_columns']}\n",
    "- Categorical Columns: {data_analysis['categorical_columns']}\n",
    "- Time Columns: {data_analysis.get('time_columns', [])}\n",
    "\n",
    "Task: Generate a feature engineering strategy for bi-weekly rate forecasting.\n",
    "\n",
    "Provide your response as a JSON with these sections:\n",
    "1. \"lag_features\": List of lag periods to create (e.g., [1, 2, 4, 8])\n",
    "2. \"rolling_features\": Rolling window calculations (e.g., [\"mean_7\", \"std_14\"])\n",
    "3. \"interaction_features\": Feature combinations to try\n",
    "4. \"time_features\": Time-based features to extract\n",
    "5. \"reasoning\": Explanation of your strategy\n",
    "\n",
    "Response:\n",
    "\"\"\"\n",
    "        return prompt\n",
    "\n",
    "    def _call_llm(self, prompt: str) -> str:\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=300,\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                pad_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        strategy_text = response.split(\"Response:\")[-1].strip()\n",
    "        return strategy_text\n",
    "\n",
    "\n",
    "    def _create_lag_features(self, data: pd.DataFrame, lag_list: list, numeric_cols: list) -> pd.DataFrame:\n",
    "        for lag in lag_list:\n",
    "            for col in numeric_cols:\n",
    "                data[f\"{col}_lag_{lag}\"] = data.groupby(level='PERMNO')[col].shift(lag)\n",
    "        return data\n",
    "\n",
    "    def _create_rolling_features(self, data: pd.DataFrame, rolling_list: list, numeric_cols: list) -> pd.DataFrame:\n",
    "        for feature in rolling_list:\n",
    "            if feature.startswith(\"mean_\"):\n",
    "                window = int(feature.split(\"_\")[1])\n",
    "                for col in numeric_cols:\n",
    "                    data[f\"{col}_rolling_mean_{window}\"] = data.groupby(level='PERMNO')[col].rolling(window=window, min_periods=1).mean().reset_index(level=0, drop=True)\n",
    "            elif feature.startswith(\"std_\"):\n",
    "                window = int(feature.split(\"_\")[1])\n",
    "                for col in numeric_cols:\n",
    "                    data[f\"{col}_rolling_std_{window}\"] = data.groupby(level='PERMNO')[col].rolling(window=window, min_periods=1).std().reset_index(level=0, drop=True)\n",
    "        return data\n",
    "\n",
    "    def _create_time_features(self, data: pd.DataFrame, time_features: list) -> pd.DataFrame:\n",
    "        # Extract time from the 'date' level in MultiIndex\n",
    "        if 'date' in data.index.names:\n",
    "            dates = data.index.get_level_values('date')\n",
    "            if \"month\" in time_features:\n",
    "                data['month'] = dates.month\n",
    "            if \"quarter\" in time_features:\n",
    "                data['quarter'] = dates.quarter\n",
    "            if \"day_of_week\" in time_features:\n",
    "                data['day_of_week'] = dates.dayofweek\n",
    "            if \"is_weekend\" in time_features:\n",
    "                data['is_weekend'] = dates.dayofweek.isin([5, 6]).astype(int)\n",
    "        return data\n",
    "\n",
    "    def process_features(self, data: pd.DataFrame, target_column: str, data_analysis: Dict) -> Dict[str, Any]:\n",
    "        prompt = self._build_prompt(data_analysis, target_column)\n",
    "        \n",
    "        # Replace mock with real LLM call when ready:\n",
    "        # strategy_text = '''\n",
    "        #     {\n",
    "        #     \"lag_features\": [1, 2, 4],\n",
    "        #     \"rolling_features\": [\"mean_7\", \"std_7\"],\n",
    "        #     \"interaction_features\": [],\n",
    "        #     \"time_features\": [\"month\", \"quarter\", \"day_of_week\", \"is_weekend\"],\n",
    "        #     \"reasoning\": \"Use recent lags, weekly rolling stats, and common time features to capture seasonality.\"\n",
    "        #     }\n",
    "        #             '''\n",
    "        strategy_text = self._call_llm(prompt)\n",
    "        print('LLM strategy')\n",
    "        print(strategy_text)\n",
    "        try:\n",
    "            strategy = json.loads(strategy_text)\n",
    "        except json.JSONDecodeError:\n",
    "            strategy = {\n",
    "                \"lag_features\": [1,2],\n",
    "                \"rolling_features\": [\"mean_7\"],\n",
    "                \"interaction_features\": [],\n",
    "                \"time_features\": [\"month\"],\n",
    "                \"reasoning\": \"Fallback strategy.\"\n",
    "            }\n",
    "\n",
    "        processed_data = data.copy()\n",
    "        \n",
    "        # Create features as per strategy\n",
    "        processed_data = self._create_lag_features(processed_data, strategy.get(\"lag_features\", []), data_analysis['numeric_columns'])\n",
    "        processed_data = self._create_rolling_features(processed_data, strategy.get(\"rolling_features\", []), data_analysis['numeric_columns'])\n",
    "        processed_data = self._create_time_features(processed_data, strategy.get(\"time_features\", []))\n",
    "\n",
    "        # Dropping NaN rows generated by lags and rolling features\n",
    "        processed_data = processed_data.dropna(how='all').fillna(0)\n",
    "\n",
    "        # Final feature names\n",
    "        feature_names = list(processed_data.columns)\n",
    "\n",
    "        return {\n",
    "            \"strategy\": strategy,\n",
    "            \"processed_data\": processed_data,\n",
    "            \"feature_names\": feature_names,\n",
    "            \"engineering_log\": f\"Applied feature engineering strategy: {strategy.get('reasoning')}\"\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac14721a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize the communicating framework\n",
    "# def create_communicating_framework():\n",
    "#     feature_engineer = CommunicatingFeatureEngineerLLM('microsoft/Phi-4-mini-instruct')\n",
    "#     forecaster = CommunicatingForecastingAgentLLM('microsoft/Phi-4-mini-instruct')\n",
    "#     orchestrator = CommunicatingOrchestrator(feature_engineer, forecaster)\n",
    "#     return orchestrator\n",
    "\n",
    "# # Run the framework\n",
    "# communicating_framework = create_communicating_framework()\n",
    "\n",
    "# # Single run with communication\n",
    "# results = communicating_framework.execute_communicating_workflow(\n",
    "#     panel_df[col], 'currentShortPositionQuantity'\n",
    "# )\n",
    "\n",
    "# print(\"Communication Log:\")\n",
    "# for log_entry in results['communication_log']:\n",
    "#     print(f\"  {log_entry['sender']} → {log_entry['receiver']}: {log_entry['type']}\")\n",
    "\n",
    "# # Iterative improvement (this is where you'll see the learning!)\n",
    "# iterative_results = communicating_framework.run_iterative_improvement(\n",
    "#     panel_df[col], 'currentShortPositionQuantity', iterations=3\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76938e1",
   "metadata": {},
   "source": [
    "### Cleaned version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ed03c5",
   "metadata": {},
   "source": [
    "##### Communication Framework\n",
    "- MessageType\n",
    "- AgentMessage\n",
    "- CommunicatingAgent\n",
    "- SimpleMessageBus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6af09a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Any, List, Optional\n",
    "from enum import Enum\n",
    "\n",
    "## Communication framework\n",
    "\n",
    "class MessageType(Enum):\n",
    "    \"\"\" Enumerates the possible types of messages exchanged by agents. \"\"\"\n",
    "\n",
    "    FEATURE_REQUEST = \"feature_request\"\n",
    "    FEATURE_RESPONSE = \"feature_response\" \n",
    "    PERFORMANCE_FEEDBACK = \"performance_feedback\"\n",
    "    MODEL_REQUEST = \"model_request\"\n",
    "    MODEL_RESPONSE = \"model_response\"\n",
    "    IMPROVEMENT_SUGGESTION = \"improvement_suggestion\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class AgentMessage:\n",
    "    \"\"\"Represents a message passed between agents in a multi-agent system.\n",
    "    \n",
    "    This dataclass encapsulates all necessary information for agent-to-agent\n",
    "    communication, including sender/receiver identification, message content,\n",
    "    and conversation tracking.\n",
    "    \n",
    "    Attributes:\n",
    "        sender (str): Identifier of the agent sending the message\n",
    "        receiver (str): Identifier of the agent receiving the message  \n",
    "        message_type (MessageType): Type/category of the message\n",
    "        content (Dict[str, Any]): The actual message payload as key-value pairs\n",
    "        conversation_id (str): Unique identifier for the conversation thread\n",
    "        timestamp (str, optional): ISO format timestamp, auto-generated if None\n",
    "    \"\"\"\n",
    "    sender: str\n",
    "    receiver: str\n",
    "    message_type: MessageType\n",
    "    content: Dict[str, Any]\n",
    "    conversation_id: str\n",
    "    timestamp: str = None\n",
    "    \n",
    "    def __post_init__(self): # Initialize values that depend on self or other values (here timestamp)\n",
    "        if self.timestamp is None:\n",
    "            self.timestamp = pd.Timestamp.now().isoformat()\n",
    "\n",
    "\n",
    "class CommunicatingAgent:\n",
    "    \"\"\"Base class for agents that can communicate with other agents in a multi-agent system.\n",
    "    \n",
    "    This abstract base class provides the foundational communication infrastructure\n",
    "    for agents operating within a coordinated multi-agent framework. It handles\n",
    "    message routing, conversation tracking, and performance memory to enable agent-to-agent\n",
    "    interactions and learning.\n",
    "    \n",
    "    The agent maintains a complete conversation history and performance memory\n",
    "    to support learning and adaptation over time. Subclasses should override\n",
    "    the process_message method to implement agent-specific message handling logic.\n",
    "    \n",
    "    Attributes:\n",
    "        message_bus: Communication hub for routing messages between agents.\n",
    "                    Set automatically when the agent is registered with the system.\n",
    "        name (str): Unique identifier for this agent within the system.\n",
    "                   Set automatically during registration.\n",
    "        conversation_history (List[AgentMessage]): Complete record of all messages\n",
    "                                                  received by this agent, ordered chronologically.\n",
    "        performance_memory (List): Storage for performance feedback and successful\n",
    "                                  interaction patterns to support learning and adaptation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.message_bus = None          # Will be set when registered\n",
    "        self.name = None                 # Will be set when registered  \n",
    "        self.conversation_history = []   # All messages this agent received\n",
    "        self.performance_memory = []     # Remember what worked well\n",
    "    \n",
    "    def receive_message(self, message: AgentMessage):\n",
    "        \"\"\"Handle incoming messages\"\"\"\n",
    "        self.conversation_history.append(message)  # Keep record\n",
    "        return self.process_message(message)       # Handle the message\n",
    "\n",
    "    def send_message(self, receiver: str, message_type: MessageType, content: Dict[str, Any], conversation_id: str = None):\n",
    "        \"\"\"Send a message to another agent\"\"\"\n",
    "        if conversation_id is None:\n",
    "            conversation_id = str(uuid.uuid4())  # Create new conversation if needed\n",
    "        \n",
    "        message = AgentMessage(\n",
    "            sender=self.name,\n",
    "            receiver=receiver,\n",
    "            message_type=message_type,\n",
    "            content=content,\n",
    "            conversation_id=conversation_id\n",
    "        )\n",
    "        return self.message_bus.send_message(message)  # Use post office to send\n",
    "    \n",
    "    def process_message(self, message: AgentMessage):\n",
    "        # Override in subclasses - each agent handles messages differently\n",
    "        pass\n",
    "\n",
    "    \n",
    "class SimpleMessageBus:\n",
    "    \"\"\"Central communication hub for managing message routing in a multi-agent system.\n",
    "    \n",
    "    The SimpleMessageBus serves as the core infrastructure component that coordinates\n",
    "    communication between agents in a multi-agent framework. It maintains an agent\n",
    "    registry, handles message routing, and tracks conversation history and performance\n",
    "    metrics for system monitoring and optimization.\n",
    "    \n",
    "    This class implements a simple publish-subscribe pattern where agents register\n",
    "    themselves to send and receive messages.\n",
    "    \n",
    "    Attributes:\n",
    "        agents (Dict[str, CommunicatingAgent]): Registry mapping agent names to \n",
    "                                               their corresponding agent instances.\n",
    "        message_history (List[AgentMessage]): Chronological record of all messages\n",
    "                                            passed through the system for debugging purposes.\n",
    "        performance_tracker (Dict): Storage for conversation performance metrics\n",
    "                                   and system optimization data.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.agents = {}                # Registry of all agents\n",
    "        self.message_history = []       # Keep track of all messages\n",
    "        self.performance_tracker = {}   # Track how well conversations went\n",
    "    \n",
    "    def register_agent(self, name: str, agent):\n",
    "        \"\"\"Add an agent to the system\"\"\"\n",
    "        self.agents[name] = agent\n",
    "        agent.message_bus = self        # Give agent access to send messages\n",
    "        agent.name = name              # Tell agent its own name\n",
    "    \n",
    "    def send_message(self, message: AgentMessage):\n",
    "        \"\"\"Deliver a message to the right agent\"\"\"\n",
    "        self.message_history.append(message)    # Keep record\n",
    "        if message.receiver in self.agents:\n",
    "            return self.agents[message.receiver].receive_message(message)\n",
    "        else:\n",
    "            print(f\"Warning: Agent {message.receiver} not found\")\n",
    "    \n",
    "    def track_performance(self, conversation_id: str, metrics: Dict[str, Any]):\n",
    "        \"\"\"Track performance metrics for a specific conversation.\n",
    "        \n",
    "        Args:\n",
    "            conversation_id (str): Unique identifier for the conversation\n",
    "            metrics (Dict[str, Any]): Performance metrics to store\n",
    "        \"\"\"\n",
    "        if conversation_id not in self.performance_tracker:\n",
    "            self.performance_tracker[conversation_id] = []\n",
    "        \n",
    "        self.performance_tracker[conversation_id].append({\n",
    "            'timestamp': pd.Timestamp.now().isoformat(),\n",
    "            'metrics': metrics\n",
    "        })\n",
    "        \n",
    "        print(f\"Tracked performance for conversation {conversation_id[:8]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f0350d",
   "metadata": {},
   "source": [
    "##### FeatureEngineeringAgent (to be replaced by Josh's agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90fc4227",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class MockFeatureEngineeringAgent:\n",
    "    \"\"\"Mock feature engineering agent for testing forecasting components\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str):\n",
    "        # Predefined strategies for different scenarios\n",
    "        self.default_strategy = {\n",
    "            \"lag_features\": [1, 2, 7, 14],  # 1-day, 2-day, 1-week, 2-week lags\n",
    "            \"rolling_features\": [\"mean_7\", \"mean_14\", \"std_7\", \"std_14\"],\n",
    "            \"interaction_features\": [\"price_volume\", \"ret_volume\"],\n",
    "            \"time_features\": [\"month\", \"quarter\", \"day_of_week\", \"is_weekend\"],\n",
    "            \"reasoning\": \"Applied standard financial time series features: recent lags, rolling statistics, and time-based features for seasonality capture.\"\n",
    "        }\n",
    "        self.model_name = model_name # For compatibility with the real feature engineer\n",
    "    \n",
    "    def _create_lag_features(self, data: pd.DataFrame, lag_list: list, numeric_cols: list) -> pd.DataFrame:\n",
    "        \"\"\"Create lagged features grouped by PERMNO\"\"\"\n",
    "        data_copy = data.copy()\n",
    "        for lag in lag_list:\n",
    "            for col in numeric_cols:\n",
    "                if col in data_copy.columns:\n",
    "                    data_copy[f\"{col}_lag_{lag}\"] = data_copy.groupby(level='PERMNO')[col].shift(lag)\n",
    "        return data_copy\n",
    "    \n",
    "    def _create_rolling_features(self, data: pd.DataFrame, rolling_list: list, numeric_cols: list) -> pd.DataFrame:\n",
    "        \"\"\"Create rolling window features grouped by PERMNO\"\"\"\n",
    "        data_copy = data.copy()\n",
    "        for feature in rolling_list:\n",
    "            feature_parts = feature.split(\"_\")\n",
    "            stat_type = feature_parts[0]\n",
    "            window = int(feature_parts[1])\n",
    "            \n",
    "            for col in numeric_cols:\n",
    "                if col in data_copy.columns:\n",
    "                    grouped = data_copy.groupby(level='PERMNO')[col].rolling(window=window, min_periods=1)\n",
    "                    if stat_type == \"mean\":\n",
    "                        data_copy[f\"{col}_rolling_mean_{window}\"] = grouped.mean().reset_index(level=0, drop=True)\n",
    "                    elif stat_type == \"std\":\n",
    "                        data_copy[f\"{col}_rolling_std_{window}\"] = grouped.std().reset_index(level=0, drop=True)\n",
    "        return data_copy\n",
    "    \n",
    "    def _create_interaction_features(self, data: pd.DataFrame, interaction_list: list) -> pd.DataFrame:\n",
    "        \"\"\"Create interaction features\"\"\"\n",
    "        data_copy = data.copy()\n",
    "        for interaction in interaction_list:\n",
    "            if interaction == \"price_volume\" and 'PRC' in data_copy.columns and 'VOL' in data_copy.columns:\n",
    "                data_copy['price_volume_interaction'] = data_copy['PRC'] * data_copy['VOL']\n",
    "            elif interaction == \"ret_volume\" and 'RET' in data_copy.columns and 'VOL' in data_copy.columns:\n",
    "                data_copy['ret_volume_interaction'] = data_copy['RET'] * data_copy['VOL']\n",
    "        return data_copy\n",
    "    \n",
    "    def _create_time_features(self, data: pd.DataFrame, time_features: list) -> pd.DataFrame:\n",
    "        \"\"\"Create time-based features from date index\"\"\"\n",
    "        data_copy = data.copy()\n",
    "        if 'date' in data_copy.index.names:\n",
    "            dates = data_copy.index.get_level_values('date')\n",
    "            if \"month\" in time_features:\n",
    "                data_copy['month'] = dates.month\n",
    "            if \"quarter\" in time_features:\n",
    "                data_copy['quarter'] = dates.quarter\n",
    "            if \"day_of_week\" in time_features:\n",
    "                data_copy['day_of_week'] = dates.dayofweek\n",
    "            if \"is_weekend\" in time_features:\n",
    "                data_copy['is_weekend'] = dates.dayofweek.isin([5, 6]).astype(int)\n",
    "        return data_copy\n",
    "    \n",
    "    def _call_llm(self, prompt: str):\n",
    "        \"\"\"Mock LLM call\"\"\"\n",
    "        return 'Text returned by LLM'\n",
    "\n",
    "    def process_features(self, data: pd.DataFrame, target_column: str, data_analysis: Dict) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Mock feature processing that applies a predefined strategy\n",
    "        \n",
    "        Args:\n",
    "            data: Input DataFrame with MultiIndex (PERMNO, date)\n",
    "            target_column: Target variable name\n",
    "            data_analysis: Data analysis dictionary (for compatibility)\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with processed data and metadata\n",
    "        \"\"\"\n",
    "        print(\"🔧 Applying mock feature engineering strategy...\")\n",
    "        \n",
    "        # Use default strategy\n",
    "        strategy = self.default_strategy.copy()\n",
    "        \n",
    "        # Apply feature engineering steps\n",
    "        processed_data = data.copy()\n",
    "        \n",
    "        # 1. Lag features\n",
    "        processed_data = self._create_lag_features(\n",
    "            processed_data, \n",
    "            strategy[\"lag_features\"], \n",
    "            data_analysis['numeric_columns']\n",
    "        )\n",
    "        \n",
    "        # 2. Rolling features  \n",
    "        processed_data = self._create_rolling_features(\n",
    "            processed_data, \n",
    "            strategy[\"rolling_features\"], \n",
    "            data_analysis['numeric_columns']\n",
    "        )\n",
    "        \n",
    "        # 3. Interaction features\n",
    "        processed_data = self._create_interaction_features(\n",
    "            processed_data, \n",
    "            strategy[\"interaction_features\"]\n",
    "        )\n",
    "        \n",
    "        # 4. Time features\n",
    "        processed_data = self._create_time_features(\n",
    "            processed_data, \n",
    "            strategy[\"time_features\"]\n",
    "        )\n",
    "        \n",
    "        # Clean up data (handle NaNs from lag/rolling operations)\n",
    "        initial_shape = processed_data.shape\n",
    "        processed_data = processed_data.dropna(how='all').fillna(0)\n",
    "        final_shape = processed_data.shape\n",
    "        \n",
    "        print(f\"✅ Mock Feature engineering complete: {initial_shape[0]} → {final_shape[0]} samples, {initial_shape[1]} → {final_shape[1]} features\")\n",
    "        \n",
    "        return {\n",
    "            \"strategy\": strategy,\n",
    "            \"processed_data\": processed_data,\n",
    "            \"feature_names\": list(processed_data.columns),\n",
    "            \"engineering_log\": f\"Mock feature engineering applied: {len(strategy['lag_features'])} lag features, {len(strategy['rolling_features'])} rolling features, {len(strategy['time_features'])} time features. Rows dropped due to NaNs: {initial_shape[0] - final_shape[0]}\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55232762",
   "metadata": {},
   "source": [
    "##### Communicating Feature Engineer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05c59fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommunicatingFeatureEngineerLLM(CommunicatingAgent, MockFeatureEngineeringAgent):\n",
    "    \"\"\"Feature engineering agent that learns from performance feedback via communication.\n",
    "    \n",
    "    This agent combines feature engineering capabilities with multi-agent communication\n",
    "    to iteratively improve feature generation strategies based on forecasting performance\n",
    "    feedback. It maintains a history of feature performance and uses this context to\n",
    "    generate enhanced feature engineering strategies for bi-weekly panel data forecasting.\n",
    "    \n",
    "    The agent inherits communication capabilities from CommunicatingAgent and feature\n",
    "    engineering logic from FeatureEngineeringAgentLLM, extending both with performance-\n",
    "    aware feature strategy generation.\n",
    "    \n",
    "    Attributes:\n",
    "        feature_performance_history (List[Dict]): Historical record of performance \n",
    "                                                 feedback including metrics, features used,\n",
    "                                                 and feature importance scores.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str):\n",
    "        CommunicatingAgent.__init__(self)\n",
    "        MockFeatureEngineeringAgent.__init__(self, model_name)\n",
    "        self.feature_performance_history = []\n",
    "    \n",
    "    def process_message(self, message: AgentMessage):\n",
    "        \"\"\"Process incoming messages based on message type.\n",
    "        \n",
    "        Routes messages to appropriate handlers based on MessageType.\n",
    "        Handles PERFORMANCE_FEEDBACK and FEATURE_REQUEST message types.\n",
    "        \n",
    "        Args:\n",
    "            message (AgentMessage): Incoming message containing type and content\n",
    "            \n",
    "        Returns:\n",
    "            Response from the appropriate message handler, or None if unhandled\n",
    "        \"\"\"\n",
    "\n",
    "        if message.message_type == MessageType.PERFORMANCE_FEEDBACK:\n",
    "            return self.handle_performance_feedback(message)\n",
    "        elif message.message_type == MessageType.FEATURE_REQUEST:\n",
    "            return self.handle_feature_request(message)\n",
    "\n",
    "\n",
    "    def handle_performance_feedback(self, message: AgentMessage):\n",
    "        \"\"\"Learn from forecasting performance to improve future feature engineering\"\"\"\n",
    "        feedback = message.content\n",
    "        \n",
    "        # Store performance history\n",
    "        self.feature_performance_history.append({\n",
    "            'timestamp': message.timestamp,\n",
    "            'metrics': feedback.get('metrics', {}),\n",
    "            'features_used': feedback.get('features_used', []),\n",
    "            'feature_importance': feedback.get('feature_importance', {}),\n",
    "            'conversation_id': message.conversation_id\n",
    "        })\n",
    "        \n",
    "        # Generate improvement suggestions\n",
    "        improvements = self._generate_improvements_from_feedback(feedback)\n",
    "        \n",
    "        # Send back suggestions\n",
    "        response = self.send_message(\n",
    "            receiver=message.sender,\n",
    "            message_type=MessageType.IMPROVEMENT_SUGGESTION,\n",
    "            content=improvements,\n",
    "            conversation_id=message.conversation_id\n",
    "        )\n",
    "        \n",
    "        return response\n",
    "    \n",
    "\n",
    "    def _generate_improvements_from_feedback(self, feedback: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Generate specific improvements based on performance feedback\"\"\"\n",
    "        metrics = feedback.get('metrics', {})\n",
    "        feature_importance = feedback.get('feature_importance', {})\n",
    "        features_used = feedback.get('features_used', [])\n",
    "        \n",
    "        # Simple rules for improvement (can be enhanced with LLM later)\n",
    "        improvements = {\n",
    "            'analysis': {},\n",
    "            'suggested_changes': {},\n",
    "            'reasoning': \"\"\n",
    "        }\n",
    "        \n",
    "        # Analyze performance\n",
    "        mae = metrics.get('MAE', float('inf'))\n",
    "        r2 = metrics.get('R2', 0)\n",
    "        \n",
    "        improvements['analysis'] = {\n",
    "            'performance_level': 'good' if r2 > 0.7 else 'moderate' if r2 > 0.3 else 'poor',\n",
    "            'mae_level': 'good' if mae < 0.1 else 'moderate' if mae < 0.5 else 'poor'\n",
    "        }\n",
    "        \n",
    "        # Feature-specific suggestions\n",
    "        if feature_importance:\n",
    "            top_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "            low_features = sorted(feature_importance.items(), key=lambda x: x[1])[:5]\n",
    "            \n",
    "            improvements['suggested_changes'] = {\n",
    "                'keep_feature_types': [f.split('_')[0] for f, _ in top_features],\n",
    "                'reduce_feature_types': [f.split('_')[0] for f, _ in low_features],\n",
    "                'increase_lags': r2 < 0.5,  # If poor performance, try more lags\n",
    "                'add_interactions': mae > 0.3  # If high error, try interactions\n",
    "            }\n",
    "        \n",
    "        improvements['reasoning'] = f\"Based on R2={r2:.3f} and MAE={mae:.3f}, suggesting feature adjustments\"\n",
    "        \n",
    "        return improvements\n",
    "\n",
    "\n",
    "    def _get_recent_performance_context(self) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"Get context from recent performance feedback\"\"\"\n",
    "        if not self.feature_performance_history:\n",
    "            return None\n",
    "        \n",
    "        # Get most recent feedback\n",
    "        recent = sorted(self.feature_performance_history, key=lambda x: x['timestamp'])[-1]\n",
    "        return recent    \n",
    " \n",
    "\n",
    "    def process_features_with_communication(self, data: pd.DataFrame, target_column: str, \n",
    "                                        data_analysis: Dict, conversation_id: str = None) -> Dict[str, Any]:\n",
    "        \"\"\"Generate features using LLM strategy enhanced with performance context.\n",
    "        \n",
    "        Creates feature engineering strategy by combining data analysis with historical\n",
    "        performance feedback. Uses LLM to generate strategy and applies feature\n",
    "        transformations including lags, rolling statistics, and time features.\n",
    "        \n",
    "        Args:\n",
    "            data (pd.DataFrame): Input panel data for feature engineering\n",
    "            target_column (str): Name of the target variable column\n",
    "            data_analysis (Dict): Data analysis summary containing shape, entities, etc.\n",
    "            conversation_id (str, optional): Conversation identifier. Auto-generated if None.\n",
    "            \n",
    "        Returns:\n",
    "            Dict[str, Any]: Dictionary containing:\n",
    "                - strategy: LLM-generated feature engineering strategy\n",
    "                - processed_data: DataFrame with engineered features\n",
    "                - feature_names: List of all feature column names\n",
    "                - engineering_log: Description of applied strategy\n",
    "                - conversation_id: Conversation identifier\n",
    "                - performance_context_used: Whether historical context was available\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        if conversation_id is None:\n",
    "            conversation_id = str(uuid.uuid4())\n",
    "        \n",
    "        # Get historical context\n",
    "        recent_performance = self._get_recent_performance_context()\n",
    "        \n",
    "        # Build enhanced prompt with performance context\n",
    "        prompt = self._build_prompt_with_context(data_analysis, target_column, recent_performance)\n",
    "        \n",
    "        # Get LLM strategy\n",
    "        strategy_text = self._call_llm(prompt)\n",
    "        \n",
    "        try:\n",
    "            strategy = json.loads(strategy_text)\n",
    "        except json.JSONDecodeError:\n",
    "            strategy = self._get_fallback_strategy(recent_performance)\n",
    "        \n",
    "        # Apply strategy\n",
    "        processed_data = data.copy()\n",
    "        processed_data = self._create_lag_features(processed_data, strategy.get(\"lag_features\", []), data_analysis['numeric_columns'])\n",
    "        processed_data = self._create_rolling_features(processed_data, strategy.get(\"rolling_features\", []), data_analysis['numeric_columns'])\n",
    "        processed_data = self._create_time_features(processed_data, strategy.get(\"time_features\", []))\n",
    "        processed_data = processed_data.dropna(how='all').fillna(0)\n",
    "        \n",
    "        return {\n",
    "            \"strategy\": strategy,\n",
    "            \"processed_data\": processed_data,\n",
    "            \"feature_names\": list(processed_data.columns),\n",
    "            \"engineering_log\": f\"Applied strategy with performance context: {strategy.get('reasoning')}\",\n",
    "            \"conversation_id\": conversation_id,\n",
    "            \"performance_context_used\": recent_performance is not None\n",
    "        }\n",
    "\n",
    "    \n",
    "    def _build_prompt_with_context(self, data_analysis: Dict, target_column: str, \n",
    "                                 performance_context: Optional[Dict[str, Any]]) -> str:\n",
    "\n",
    "        \"\"\"Build LLM prompt incorporating performance context.\n",
    "        \n",
    "        Args:\n",
    "            context: Performance context information\n",
    "            \n",
    "        Returns:\n",
    "            str: Enhanced prompt string with context\n",
    "        \"\"\"\n",
    "        \n",
    "        context_section = \"\"\n",
    "        if performance_context:\n",
    "            metrics = performance_context['metrics']\n",
    "            top_features = list(performance_context.get('feature_importance', {}).keys())[:5]\n",
    "            \n",
    "            context_section = f\"\"\"\n",
    "Previous Performance Context:\n",
    "- Last R2: {metrics.get('R2', 'N/A'):.3f}\n",
    "- Last MAE: {metrics.get('MAE', 'N/A'):.3f}\n",
    "- Top performing features: {top_features}\n",
    "\n",
    "Use this context to improve the strategy.\n",
    "\"\"\"\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "You are a feature engineering expert for bi-weekly forecasting on panel data.\n",
    "\n",
    "Data Analysis:\n",
    "- Shape: {data_analysis['shape']}\n",
    "- Target: {target_column} (bi-weekly frequency)\n",
    "- Entities: {data_analysis.get('entities', 'Unknown')}\n",
    "- Date range: {data_analysis.get('date_range', 'Unknown')}\n",
    "\n",
    "{context_section}\n",
    "\n",
    "Task: Create an improved feature engineering strategy.\n",
    "Focus on bi-weekly patterns and cross-sectional variations.\n",
    "\n",
    "Return JSON with:\n",
    "1. \"lag_features\": [1, 2, 4, 8, 14] (include bi-weekly relevant lags)\n",
    "2. \"rolling_features\": [\"mean_7\", \"mean_14\", \"std_14\"]  \n",
    "3. \"interaction_features\": []\n",
    "4. \"time_features\": [\"month\", \"quarter\", \"day_of_week\"]\n",
    "5. \"reasoning\": Your strategy explanation\n",
    "\n",
    "Response:\n",
    "\"\"\"\n",
    "        return prompt\n",
    "    \n",
    "\n",
    "    def _get_fallback_strategy(self, performance_context: Optional[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        \"\"\"Fallback strategy based on performance context\"\"\"\n",
    "        if performance_context and performance_context['metrics'].get('R2', 0) < 0.3:\n",
    "            # Poor performance - try more features\n",
    "            return {\n",
    "                \"lag_features\": [1, 2, 4, 8, 14],\n",
    "                \"rolling_features\": [\"mean_7\", \"mean_14\", \"std_7\", \"std_14\"],\n",
    "                \"interaction_features\": [],\n",
    "                \"time_features\": [\"month\", \"quarter\", \"day_of_week\", \"is_weekend\"],\n",
    "                \"reasoning\": \"Expanding feature set due to poor previous performance\"\n",
    "            }\n",
    "        else:\n",
    "            # Default strategy\n",
    "            return {\n",
    "                \"lag_features\": [1, 2, 4],\n",
    "                \"rolling_features\": [\"mean_7\"],\n",
    "                \"interaction_features\": [],\n",
    "                \"time_features\": [\"month\"],\n",
    "                \"reasoning\": \"Standard fallback strategy\"\n",
    "            }    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b77a89",
   "metadata": {},
   "source": [
    "#### Forecasting Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8064537",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, List, Tuple, Any\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import optuna\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class ForecastingAgentLLM:\n",
    "    \"\"\"XGBoost forecasting agent with Optuna hyperparameter optimization and optional LLM integration.\n",
    "    \n",
    "    This agent performs bi-weekly financial forecasting on panel data using XGBoost regression\n",
    "    with automated hyperparameter tuning via Optuna. It includes time series cross-validation,\n",
    "    feature scaling, and comprehensive performance metrics calculation.\n",
    "    \n",
    "    The agent can optionally integrate with Large Language Models for enhanced feature\n",
    "    engineering and strategy generation when initialized with a model_name.\n",
    "    \n",
    "    Attributes:\n",
    "        model (xgb.XGBRegressor): XGBoost regression model for forecasting\n",
    "        scaler (StandardScaler): Feature scaling transformer\n",
    "        best_params (Dict, optional): Optimized hyperparameters from Optuna trials\n",
    "        model_name (str, optional): Hugging Face model identifier for LLM integration\n",
    "        tokenizer (AutoTokenizer, optional): Tokenizer for LLM processing\n",
    "        model_llm (AutoModelForCausalLM, optional): LLM model for text generation\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self, model_name: str = None):\n",
    "        self.model = xgb.XGBRegressor(random_state=42, verbosity=0)\n",
    "        self.scaler = StandardScaler()\n",
    "        self.best_params = None\n",
    "        self.model_name = model_name\n",
    "\n",
    "        # Initialize LLM components if provided\n",
    "        if model_name:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "            self.model_llm = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "            if self.tokenizer.pad_token is None:\n",
    "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "    def _prepare_biweekly_data(self, data: pd.DataFrame, target_column: str) -> pd.DataFrame:\n",
    "        \"\"\"Extract bi-weekly target observation points while preserving daily feature data.\n",
    "        \n",
    "        Identifies dates where the target variable actually changed to create training\n",
    "        samples at bi-weekly frequency, avoiding duplicate target values while maintaining\n",
    "        the full feature set for each entity.\n",
    "        \n",
    "        Args:\n",
    "            data (pd.DataFrame): Panel data with ('PERMNO', 'date') MultiIndex\n",
    "            target_column (str): Name of bi-weekly target variable column\n",
    "            \n",
    "        Returns:\n",
    "            pd.DataFrame: Filtered data containing only bi-weekly target change points\n",
    "            \n",
    "        Note:\n",
    "            Handles duplicate indices and maintains entity-level data integrity.\n",
    "            Prints warnings for processing errors but continues with valid entities.\n",
    "        \"\"\"\n",
    "\n",
    "        # if data.index.names[0] != 'PERMNO':\n",
    "        #     data_sorted = data.swaplevel().sort_index()\n",
    "        # else:\n",
    "        #     data_sorted = data.sort_index()\n",
    "\n",
    "        # if not data_sorted.index.is_unique:\n",
    "        #     print(f\"Warning: Found {data_sorted.index.duplicated().sum()} duplicate index entries. Removing duplicates.\")\n",
    "        #     data_sorted = data_sorted[~data_sorted.index.duplicated(keep='last')]\n",
    "\n",
    "        # biweekly_data = []\n",
    "        # unique_permnos = data_sorted.index.get_level_values('PERMNO').unique()\n",
    "\n",
    "        # for permno in unique_permnos:\n",
    "        #     try:\n",
    "        #         entity_data = data_sorted.loc[permno]\n",
    "\n",
    "        #         if isinstance(entity_data, pd.Series):\n",
    "        #             entity_data = entity_data.to_frame().T\n",
    "        #             entity_data.index = [data_sorted.loc[permno].name[1]] if hasattr(data_sorted.loc[permno].name, '__getitem__') else [entity_data.index[0]]\n",
    "\n",
    "        #         # Get dates where target actually changed (bi-weekly points)\n",
    "        #         target_changes = entity_data[target_column] != entity_data[target_column].shift(1)\n",
    "        #         biweekly_points = entity_data[target_changes | (entity_data.index == entity_data.index[0])]\n",
    "\n",
    "        #         # Add PERMNO back to index\n",
    "        #         biweekly_points_indexed = biweekly_points.copy()\n",
    "        #         biweekly_points_indexed['PERMNO'] = permno\n",
    "        #         biweekly_points_indexed = biweekly_points_indexed.set_index('PERMNO', append=True).swaplevel()\n",
    "\n",
    "        #         biweekly_data.append(biweekly_points_indexed)\n",
    "\n",
    "        #     except (KeyError, Exception) as e:\n",
    "        #         print(f\"Error processing PERMNO {permno}: {e}\")\n",
    "        #         continue\n",
    "\n",
    "        # return pd.concat(biweekly_data) if biweekly_data else pd.DataFrame()\n",
    "\n",
    "\n",
    "        #### CHANGED TO THIS IF WE ONLY USE BI-WEEKLY DATA FOR NOW\n",
    "        df = data.sort_index(level=['date','PERMNO'])\n",
    "        return df[df[target_column].notna()]\n",
    "    \n",
    "    def _calculate_metrics(self, y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:\n",
    "        \"\"\"Calculate comprehensive evaluation metrics\"\"\"\n",
    "        try:\n",
    "            mae = mean_absolute_error(y_true, y_pred)\n",
    "            rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "            # Avoid division by zero in MAPE calculation\n",
    "            mask = y_true != 0\n",
    "            if np.sum(mask) > 0:\n",
    "                mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
    "            else:\n",
    "                mape = float('inf')\n",
    "\n",
    "            # R-squared\n",
    "            ss_res = np.sum((y_true - y_pred) ** 2)\n",
    "            ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "            r2 = 1 - (ss_res / ss_tot) if ss_tot != 0 else 0\n",
    "\n",
    "            return {\n",
    "                'MAE': float(mae),\n",
    "                'RMSE': float(rmse), \n",
    "                'MAPE': float(mape),\n",
    "                'R2': float(r2)\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating metrics: {e}\")\n",
    "            return {'MAE': float('inf'), 'RMSE': float('inf'), 'MAPE': float('inf'), 'R2': 0}\n",
    "\n",
    "    def _prepare_features_and_target(self, data: pd.DataFrame, target_column: str):\n",
    "        \"\"\"Prepare features and target for training\"\"\"\n",
    "        feature_cols = [c for c in data.columns if c != target_column]\n",
    "        X_df = data[feature_cols].select_dtypes(include=[np.number]).copy().fillna(0)\n",
    "        y = data[target_column].values\n",
    "        mask = ~np.isnan(y)\n",
    "        X = X_df.values[mask]\n",
    "        y = y[mask]\n",
    "        feature_names = X_df.columns.tolist()\n",
    "        row_index = data.index[mask]\n",
    "        return X, y, feature_names, row_index\n",
    "\n",
    "\n",
    "    def _get_xgboost_param_space(self) -> Dict:\n",
    "        \"\"\"Optimized XGBoost hyperparameter space for financial data\"\"\"\n",
    "        return {\n",
    "            'n_estimators': (50, 200),      # Reasonable range for quick training\n",
    "            'max_depth': (3, 6),            # Prevent overfitting\n",
    "            'learning_rate': (0.05, 0.2),   # Conservative learning rates\n",
    "            'subsample': (0.7, 1.0),        # Sample fraction\n",
    "            'colsample_bytree': (0.7, 1.0), # Feature fraction\n",
    "            'reg_alpha': (0, 0.5),          # L1 regularization\n",
    "            'reg_lambda': (0, 1.0),         # L2 regularization\n",
    "            'min_child_weight': (1, 5)      # Minimum samples in leaf\n",
    "        }\n",
    "\n",
    "\n",
    "    def _date_based_splits(index: pd.MultiIndex,\n",
    "                        n_splits: int = 3,\n",
    "                        purge: int = 0,\n",
    "                        expanding: bool = True):\n",
    "        \"\"\"\n",
    "        Build leakage-safe CV folds on a panel with MultiIndex ('PERMNO','date').\n",
    "\n",
    "        index: MultiIndex with a 'date' level\n",
    "        n_splits: number of validation folds\n",
    "        purge: number of unique dates to skip between train end and val start\n",
    "        expanding: if True, expanding train window; else rolling window of size ~1 fold\n",
    "        \"\"\"\n",
    "        if 'date' not in index.names:\n",
    "            raise ValueError(\"Index must contain a 'date' level\")\n",
    "        # Ensure stable order: sort by ['date','PERMNO']\n",
    "        if list(index.names) != ['PERMNO', 'date']:\n",
    "            df_tmp = pd.DataFrame(index=index).reset_index().set_index(['PERMNO','date']).sort_index()\n",
    "            index = df_tmp.index\n",
    "        else:\n",
    "            index = index.swaplevel().sort_values()  # ('date','PERMNO')\n",
    "\n",
    "        udates = np.array(sorted(index.get_level_values('date').unique()))\n",
    "        if n_splits < 1:\n",
    "            raise ValueError(\"n_splits must be >= 1\")\n",
    "        if len(udates) < (n_splits + 1):\n",
    "            raise ValueError(\"Not enough unique dates for the requested n_splits\")\n",
    "\n",
    "        fold_size = len(udates) // (n_splits + 1)\n",
    "        dates_vals = index.get_level_values('date')\n",
    "        splits = []\n",
    "        for i in range(n_splits):\n",
    "            # Train window\n",
    "            train_start_idx = 0 if expanding else i * fold_size\n",
    "            train_end_idx = (i + 1) * fold_size - 1\n",
    "            # Validation window (with purge gap)\n",
    "            val_start_idx = (i + 1) * fold_size + purge\n",
    "            val_end_idx = min((i + 2) * fold_size, len(udates) - 1)\n",
    "            if val_start_idx > val_end_idx:\n",
    "                break\n",
    "\n",
    "            train_start, train_end = udates[train_start_idx], udates[train_end_idx]\n",
    "            val_start, val_end = udates[val_start_idx], udates[val_end_idx]\n",
    "\n",
    "            train_mask = (dates_vals >= train_start) & (dates_vals <= train_end)\n",
    "            val_mask = (dates_vals >= val_start) & (dates_vals <= val_end)\n",
    "\n",
    "            train_idx = np.where(train_mask)\n",
    "            val_idx = np.where(val_mask)\n",
    "            if len(train_idx) == 0 or len(val_idx) == 0:\n",
    "                continue\n",
    "\n",
    "            splits.append((train_idx, val_idx))\n",
    "        return splits\n",
    "\n",
    "\n",
    "    def optimize_and_forecast(self, data: pd.DataFrame, target_column: str, \n",
    "                             n_trials: int = 20, cv_splits: int = 3, purge: int = 0) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Main method: Optimize XGBoost hyperparameters and generate forecasts\n",
    "\n",
    "        Args:\n",
    "            data: Panel DataFrame with engineered features\n",
    "            target_column: Target variable name\n",
    "            n_trials: Number of Optuna trials for optimization\n",
    "            cv_splits: Number of time series CV splits\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with optimization results, metrics, and forecasts\n",
    "        \"\"\"\n",
    "        print(\"Starting XGBoost optimization and forecasting...\")\n",
    "\n",
    "        # Prepare bi-weekly data\n",
    "        biweekly_data = self._prepare_biweekly_data(data, target_column)\n",
    "\n",
    "        if len(biweekly_data) < 20:\n",
    "            return {\"error\": \"Insufficient bi-weekly data for optimization\"}\n",
    "\n",
    "        # Prepare features and target\n",
    "        X, y, feature_names, row_index = self._prepare_features_and_target(biweekly_data, target_column)\n",
    "\n",
    "        # Build date-based splits from the aligned index\n",
    "        splits = self._date_based_splits(row_index, n_splits=cv_splits, purge=purge, expanding=True)\n",
    "\n",
    "        print(f\"Training data: {len(X)} samples, {len(feature_names)} features\")\n",
    "\n",
    "        # Create Optuna study with TPE sampler for efficiency\n",
    "        study = optuna.create_study(direction='maximize',\n",
    "                                    sampler=optuna.samplers.TPESampler(seed=42, n_startup_trials=5))\n",
    "\n",
    "        def objective(trial):\n",
    "            params = {}\n",
    "            for name, rng in self._get_xgboost_param_space().items():\n",
    "                if isinstance(rng, int):\n",
    "                    params[name] = trial.suggest_int(name, rng, rng[1])\n",
    "                else:\n",
    "                    params[name] = trial.suggest_float(name, rng, rng[1])\n",
    "\n",
    "            scores = []\n",
    "            for train_idx, val_idx in splits:\n",
    "                scaler = StandardScaler()\n",
    "                X_train = scaler.fit_transform(X[train_idx])\n",
    "                X_val = scaler.transform(X[val_idx])\n",
    "                model = xgb.XGBRegressor(random_state=42, verbosity=0, **params)\n",
    "                model.fit(X_train, y[train_idx])\n",
    "                y_pred = model.predict(X_val)\n",
    "                scores.append(-mean_absolute_error(y[val_idx], y_pred))  # negative MAE\n",
    "            return float(np.mean(scores))\n",
    "\n",
    "        study.optimize(objective, n_trials=n_trials, show_progress_bar=False)\n",
    "\n",
    "        # Get best parameters\n",
    "        best_params = study.best_params\n",
    "        print(f\"Best parameters found: {best_params}\")\n",
    "\n",
    "        # Train final model with best parameters and full data\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        self.model = xgb.XGBRegressor(random_state=42, verbosity=0, **best_params)\n",
    "        self.model.fit(X_scaled, y)\n",
    "        self.best_params = best_params\n",
    "\n",
    "        # Calculate cross-validation metrics with best parameters\n",
    "        tscv = TimeSeriesSplit(n_splits=cv_splits)\n",
    "        cv_scores = []\n",
    "\n",
    "        for train_idx, val_idx in tscv.split(X):\n",
    "            X_train_cv, X_val_cv = X[train_idx], X[val_idx]\n",
    "            y_train_cv, y_val_cv = y[train_idx], y[val_idx]\n",
    "\n",
    "            scaler_cv = StandardScaler()\n",
    "            X_train_scaled = scaler_cv.fit_transform(X_train_cv)\n",
    "            X_val_scaled = scaler_cv.transform(X_val_cv)\n",
    "\n",
    "            model_cv = xgb.XGBRegressor(random_state=42, verbosity=0, **best_params)\n",
    "            model_cv.fit(X_train_scaled, y_train_cv)\n",
    "            y_pred_cv = model_cv.predict(X_val_scaled)\n",
    "\n",
    "            metrics = self._calculate_metrics(y_val_cv, y_pred_cv)\n",
    "            cv_scores.append(metrics)\n",
    "\n",
    "        # Calculate average metrics\n",
    "        avg_metrics = {}\n",
    "        for metric in ['MAE', 'RMSE', 'MAPE', 'R2']:\n",
    "            values = [score[metric] for score in cv_scores if not np.isinf(score[metric])]\n",
    "            avg_metrics[f'avg_{metric}'] = np.mean(values) if values else float('inf')\n",
    "            avg_metrics[f'std_{metric}'] = np.std(values) if values else 0\n",
    "\n",
    "        # Generate predictions for latest data (next period forecast)\n",
    "        latest_data = data.groupby(level='PERMNO').tail(1)\n",
    "        X_latest, _, _ = self._prepare_features_and_target(latest_data, target_column)\n",
    "        X_latest_scaled = self.scaler.transform(X_latest)\n",
    "        future_predictions = self.model.predict(X_latest_scaled)\n",
    "\n",
    "        # Get feature importance\n",
    "        feature_importance = dict(zip(feature_names, self.model.feature_importances_))\n",
    "        top_features = dict(sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)[:10])\n",
    "\n",
    "        return {\n",
    "            'optimization_results': {\n",
    "                'best_params': best_params,\n",
    "                'best_cv_score': study.best_value,\n",
    "                'n_trials_completed': len(study.trials),\n",
    "                'optimization_time': 'Fast (< 5 minutes typical)'\n",
    "            },\n",
    "            'cross_validation_metrics': {\n",
    "                'average_metrics': avg_metrics,\n",
    "                'individual_fold_scores': cv_scores,\n",
    "                'num_cv_folds': cv_splits\n",
    "            },\n",
    "            'forecasts': {\n",
    "                'next_period_predictions': future_predictions.tolist(),\n",
    "                'num_entities_forecasted': len(future_predictions),\n",
    "                'training_samples_used': len(biweekly_data)\n",
    "            },\n",
    "            'model_insights': {\n",
    "                'feature_count': len(feature_names),\n",
    "                'top_feature_importance': top_features,\n",
    "                'model_type': 'XGBoost Regressor'\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def predict_latest(self, data: pd.DataFrame, target_column: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate predictions for the latest data using trained model\n",
    "\n",
    "        Args:\n",
    "            data: Panel DataFrame with features\n",
    "            target_column: Target variable name\n",
    "\n",
    "        Returns:\n",
    "            Array of predictions for latest period\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained. Run optimize_and_forecast() first.\")\n",
    "\n",
    "        latest_data = data.groupby(level='PERMNO').tail(1)\n",
    "        X_latest, _, _ = self._prepare_features_and_target(latest_data, target_column)\n",
    "        X_latest_scaled = self.scaler.transform(X_latest)\n",
    "\n",
    "        return self.model.predict(X_latest_scaled)\n",
    "\n",
    "    def get_model_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get summary of the trained model\"\"\"\n",
    "        if self.best_params is None:\n",
    "            return {\"error\": \"Model not trained yet\"}\n",
    "\n",
    "        return {\n",
    "            'model_type': 'XGBoost Regressor',\n",
    "            'best_hyperparameters': self.best_params,\n",
    "            'is_trained': self.model is not None,\n",
    "            'feature_scaler': 'StandardScaler applied'\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9127f8d9",
   "metadata": {},
   "source": [
    "#### CommunicatingForecastingAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e17ccee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommunicatingForecastingAgent(CommunicatingAgent, ForecastingAgentLLM):\n",
    "    \"\"\"Forecasting agent that provides performance feedback to feature engineers via communication.\n",
    "    \n",
    "    This agent combines XGBoost forecasting capabilities with multi-agent communication\n",
    "    to create a feedback loop for feature engineering optimization. After running\n",
    "    forecasts with hyperparameter optimization, it automatically sends performance\n",
    "    metrics and feature importance data to feature engineering agents.\n",
    "    \n",
    "    The agent inherits forecasting logic from ForecastingAgentLLM and communication\n",
    "    capabilities from CommunicatingAgent, enabling coordinated learning in multi-agent\n",
    "    quantitative finance systems.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str = None):\n",
    "        CommunicatingAgent.__init__(self)\n",
    "        ForecastingAgentLLM.__init__(self, model_name)\n",
    "    \n",
    "    def validate_with_feedback(self, data: pd.DataFrame, target_column: str, conversation_id: str, n_trials: int=20, cv_splits: int=3) -> Dict[str, Any]:\n",
    "        \"\"\"Run forecasting validation and send performance feedback to feature engineers.\n",
    "        \n",
    "        Performs XGBoost hyperparameter optimization and cross-validation, then automatically\n",
    "        sends comprehensive performance feedback to the feature engineering agent via the\n",
    "        message bus. This enables continuous improvement of feature engineering strategies.\n",
    "        \n",
    "        Args:\n",
    "            data (pd.DataFrame): Panel data with engineered features and target variable\n",
    "            target_column (str): Name of the target variable column for forecasting\n",
    "            conversation_id (str): Unique identifier for tracking the conversation thread\n",
    "            n_trials (int): Number of Optuna hyperparameter optimization trials\n",
    "            cv_splits (int): Number of time series cross-validation splits\n",
    "            \n",
    "        Returns:\n",
    "            Dict[str, Any]: Complete forecasting results including:\n",
    "                - optimization_results: Best parameters and optimization metrics\n",
    "                - cross_validation_metrics: Performance metrics across CV folds\n",
    "                - forecasts: Next period predictions for all entities\n",
    "                - model_insights: Feature importance and model characteristics\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        # Run original validation\n",
    "        results = self.optimize_and_forecast(data, target_column, n_trials = n_trials, cv_splits= cv_splits)\n",
    "        \n",
    "        # Extract feedback information\n",
    "        if 'cross_validation_metrics' in results:\n",
    "            feedback_content = {\n",
    "                'metrics': results['cross_validation_metrics'].get('average_metrics', {}),\n",
    "                'features_used': list(data.columns),\n",
    "                'feature_importance': results['model_insights'].get('top_feature_importance', {}),\n",
    "                'model_used': results['model_insights'].get('model_type'),\n",
    "                'best_parameters': results['optimization_results'].get('best_params'),\n",
    "                'best_cv_score': results['optimization_results'].get('best_cv_score'),\n",
    "                'conversation_id': conversation_id\n",
    "            }\n",
    "\n",
    "            # Send feedback to feature engineer\n",
    "            self.send_message(\n",
    "                receiver='feature_engineer',\n",
    "                message_type=MessageType.PERFORMANCE_FEEDBACK,\n",
    "                content=feedback_content,\n",
    "                conversation_id=conversation_id\n",
    "            )\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2d2fbb",
   "metadata": {},
   "source": [
    "#### Orchestrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "516e277b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommunicatingOrchestrator(CommunicatingAgent):\n",
    "    \"\"\"Orchestrator that coordinates multi-agent feature engineering and forecasting workflows.\n",
    "    \n",
    "    This orchestrator manages communication between feature engineering and forecasting agents\n",
    "    to create an iterative improvement loop for quantitative finance modeling. It sets up\n",
    "    the message bus, registers agents, and coordinates the complete workflow from data\n",
    "    analysis through forecasting with performance feedback.\n",
    "    \n",
    "    The orchestrator tracks conversation history and enables iterative model improvement\n",
    "    through agent-to-agent communication and performance monitoring.\n",
    "    \n",
    "    Attributes:\n",
    "        feature_engineer (CommunicatingFeatureEngineerLLM): Feature engineering agent\n",
    "        forecaster (CommunicatingForecastingAgentLLM): Forecasting agent  \n",
    "        message_bus (SimpleMessageBus): Communication hub for agent messaging\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, feature_engineer: CommunicatingFeatureEngineerLLM, \n",
    "                 forecaster: CommunicatingForecastingAgent):\n",
    "        super().__init__()\n",
    "        self.feature_engineer = feature_engineer\n",
    "        self.forecaster = forecaster\n",
    "        self.message_bus = SimpleMessageBus()\n",
    "        \n",
    "        # Register agents\n",
    "        self.message_bus.register_agent('orchestrator', self)\n",
    "        self.message_bus.register_agent('feature_engineer', feature_engineer)\n",
    "        self.message_bus.register_agent('forecaster', forecaster)\n",
    "    \n",
    "    def execute_communicating_workflow(self, data: pd.DataFrame, target_column: str) -> Dict[str, Any]:\n",
    "        \"\"\"Execute complete multi-agent workflow with feature engineering and forecasting.\n",
    "        \n",
    "        Coordinates a full machine learning pipeline using communicating agents:\n",
    "        1. Performs comprehensive data analysis\n",
    "        2. Executes feature engineering with historical performance context\n",
    "        3. Runs forecasting with hyperparameter optimization\n",
    "        4. Sends performance feedback for continuous improvement\n",
    "        5. Tracks communication history for workflow transparency\n",
    "        \n",
    "        Args:\n",
    "            data (pd.DataFrame): Panel data with ('PERMNO', 'date') MultiIndex\n",
    "            target_column (str): Name of the target variable for forecasting\n",
    "            \n",
    "        Returns:\n",
    "            Dict[str, Any]: Comprehensive workflow results containing:\n",
    "                - conversation_id: Unique identifier for this workflow execution\n",
    "                - data_analysis: Summary statistics and data characteristics\n",
    "                - engineered_features: Feature engineering results and strategy used\n",
    "                - forecast_results: Complete forecasting results with metrics\n",
    "                - communication_log: Record of all agent-to-agent messages\n",
    "        \"\"\"\n",
    "\n",
    "        conversation_id = str(uuid.uuid4())\n",
    "        \n",
    "        print(f\"Starting communicating workflow (ID: {conversation_id[:8]}...)\")\n",
    "        \n",
    "        # Step 1: Data Analysis\n",
    "        data_analysis = {\n",
    "            \"shape\": data.shape,\n",
    "            \"columns\": list(data.columns),\n",
    "            \"target_column\": target_column,\n",
    "            \"numeric_columns\": list(data.select_dtypes(include=['number']).columns),\n",
    "            \"categorical_columns\": list(data.select_dtypes(include=['object', 'category']).columns),\n",
    "            \"date_range\": (data.index.get_level_values('date').min(), data.index.get_level_values('date').max()),\n",
    "            \"entities\": len(data.index.get_level_values('PERMNO').unique())\n",
    "        }\n",
    "        \n",
    "        # Step 2: Feature Engineering with Communication\n",
    "        print(\"Feature Engineering Agent working...\")\n",
    "        engineered_data = self.feature_engineer.process_features_with_communication(\n",
    "            data, target_column, data_analysis, conversation_id\n",
    "        )\n",
    "        \n",
    "        # Step 3: Forecasting with Feedback\n",
    "        print(\"Forecasting Agent working...\")\n",
    "        forecast_results = self.forecaster.validate_with_feedback(\n",
    "            engineered_data[\"processed_data\"], target_column, conversation_id, n_trials=20, cv_splits=3\n",
    "        )\n",
    "        \n",
    "        # Step 4: Track overall performance\n",
    "        if 'cross_validation_metrics' in forecast_results:\n",
    "            metrics = forecast_results['cross_validation_metrics'].get('average_metrics', {})\n",
    "            self.message_bus.track_performance(conversation_id, metrics)\n",
    "        \n",
    "        return {\n",
    "            \"conversation_id\": conversation_id,\n",
    "            \"data_analysis\": data_analysis,\n",
    "            \"engineered_features\": engineered_data,\n",
    "            \"forecast_results\": forecast_results,\n",
    "            \"communication_log\": [\n",
    "                {\n",
    "                    \"sender\": msg.sender,\n",
    "                    \"receiver\": msg.receiver, \n",
    "                    \"type\": msg.message_type.value,\n",
    "                    \"timestamp\": msg.timestamp\n",
    "                }\n",
    "                for msg in self.message_bus.message_history \n",
    "                if msg.conversation_id == conversation_id\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    \n",
    "    def run_iterative_improvement(self, data: pd.DataFrame, target_column: str, iterations: int = 3):\n",
    "        \"\"\"Execute multiple workflow iterations to demonstrate continuous improvement.\n",
    "        \n",
    "        Runs the complete communicating workflow multiple times to show how agents\n",
    "        learn and improve performance through feedback loops. Each iteration builds\n",
    "        on the performance history from previous runs.\n",
    "        \n",
    "        Args:\n",
    "            data (pd.DataFrame): Panel data with ('PERMNO', 'date') MultiIndex\n",
    "            target_column (str): Name of the target variable for forecasting\n",
    "            iterations (int, optional): Number of improvement iterations. Defaults to 3.\n",
    "            \n",
    "        Returns:\n",
    "            List[Dict[str, Any]]: List of workflow results for each iteration,\n",
    "                                enabling performance comparison across iterations\n",
    "        \"\"\"\n",
    "\n",
    "        results = []\n",
    "        \n",
    "        for i in range(iterations):\n",
    "            print(f\"\\n=== Iteration {i+1}/{iterations} ===\")\n",
    "            result = self.execute_communicating_workflow(data, target_column)\n",
    "            results.append(result)\n",
    "            \n",
    "            # Print progress\n",
    "            if 'forecast_results' in result and 'validation_results' in result['forecast_results']:\n",
    "                metrics = result['forecast_results']['validation_results'].get('test_metrics', {})\n",
    "                print(f\"Iteration {i+1} - R2: {metrics.get('R2', 'N/A'):.3f}, MAE: {metrics.get('MAE', 'N/A'):.3f}\")\n",
    "        \n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71c8bf1",
   "metadata": {},
   "source": [
    "#### Run the framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b914625",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-15 21:05:13,385] A new study created in memory with name: no-name-704aea05-1ef6-4753-9ee1-14da67a177eb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting communicating workflow (ID: 110d53e6...)\n",
      "Feature Engineering Agent working...\n",
      "Forecasting Agent working...\n",
      "Starting XGBoost optimization and forecasting...\n",
      "Training data: 105 samples, 30 features\n",
      "Running 20 optimization trials...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-15 21:05:13,725] Trial 0 finished with value: -13.255250295003256 and parameters: {'n_estimators': 106, 'max_depth': 6, 'learning_rate': 0.15979909127171077, 'subsample': 0.8795975452591109, 'colsample_bytree': 0.7468055921327309, 'reg_alpha': 0, 'reg_lambda': 0, 'min_child_weight': 1}. Best is trial 0 with value: -13.255250295003256.\n",
      "[I 2025-09-15 21:05:14,235] Trial 1 finished with value: -13.090790589650473 and parameters: {'n_estimators': 180, 'max_depth': 5, 'learning_rate': 0.15621088666940683, 'subsample': 0.7061753482887407, 'colsample_bytree': 0.9909729556485983, 'reg_alpha': 0, 'reg_lambda': 1, 'min_child_weight': 2}. Best is trial 1 with value: -13.090790589650473.\n",
      "[I 2025-09-15 21:05:14,395] Trial 2 finished with value: -12.013821919759115 and parameters: {'n_estimators': 77, 'max_depth': 3, 'learning_rate': 0.09563633644393067, 'subsample': 0.8574269294896714, 'colsample_bytree': 0.8295835055926347, 'reg_alpha': 0, 'reg_lambda': 0, 'min_child_weight': 4}. Best is trial 2 with value: -12.013821919759115.\n",
      "[I 2025-09-15 21:05:14,586] Trial 3 finished with value: -12.29013999303182 and parameters: {'n_estimators': 71, 'max_depth': 4, 'learning_rate': 0.10495427649405377, 'subsample': 0.8368209952651108, 'colsample_bytree': 0.9355527884179041, 'reg_alpha': 0, 'reg_lambda': 0, 'min_child_weight': 3}. Best is trial 2 with value: -12.013821919759115.\n",
      "[I 2025-09-15 21:05:14,866] Trial 4 finished with value: -13.282195091247559 and parameters: {'n_estimators': 139, 'max_depth': 3, 'learning_rate': 0.14113172778521577, 'subsample': 0.7511572371061874, 'colsample_bytree': 0.7195154778955838, 'reg_alpha': 0, 'reg_lambda': 1, 'min_child_weight': 5}. Best is trial 2 with value: -12.013821919759115.\n",
      "[I 2025-09-15 21:05:14,982] Trial 5 finished with value: -12.970125198364258 and parameters: {'n_estimators': 53, 'max_depth': 3, 'learning_rate': 0.05113209789744397, 'subsample': 0.9796871025735362, 'colsample_bytree': 0.8346612033587996, 'reg_alpha': 0, 'reg_lambda': 0, 'min_child_weight': 5}. Best is trial 2 with value: -12.013821919759115.\n",
      "[I 2025-09-15 21:05:15,260] Trial 6 finished with value: -11.30441951751709 and parameters: {'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.08472528082300293, 'subsample': 0.8771424068781188, 'colsample_bytree': 0.8392938332598474, 'reg_alpha': 0, 'reg_lambda': 0, 'min_child_weight': 4}. Best is trial 6 with value: -11.30441951751709.\n",
      "[I 2025-09-15 21:05:15,695] Trial 7 finished with value: -10.702038764953613 and parameters: {'n_estimators': 137, 'max_depth': 5, 'learning_rate': 0.19753690038993088, 'subsample': 0.9625659107838825, 'colsample_bytree': 0.8934598467451235, 'reg_alpha': 0, 'reg_lambda': 1, 'min_child_weight': 3}. Best is trial 7 with value: -10.702038764953613.\n",
      "[I 2025-09-15 21:05:16,194] Trial 8 finished with value: -11.881921609242758 and parameters: {'n_estimators': 154, 'max_depth': 6, 'learning_rate': 0.18663270786035432, 'subsample': 0.994287975478448, 'colsample_bytree': 0.9104585872663942, 'reg_alpha': 0, 'reg_lambda': 1, 'min_child_weight': 2}. Best is trial 7 with value: -10.702038764953613.\n",
      "[I 2025-09-15 21:05:16,824] Trial 9 finished with value: -10.722438017527262 and parameters: {'n_estimators': 196, 'max_depth': 5, 'learning_rate': 0.19990436579623266, 'subsample': 0.9373999610946279, 'colsample_bytree': 0.901109668141453, 'reg_alpha': 0, 'reg_lambda': 1, 'min_child_weight': 3}. Best is trial 7 with value: -10.702038764953613.\n",
      "[I 2025-09-15 21:05:17,247] Trial 10 finished with value: -12.391599496205648 and parameters: {'n_estimators': 154, 'max_depth': 5, 'learning_rate': 0.17618649279565268, 'subsample': 0.9241899536391185, 'colsample_bytree': 0.789866541852453, 'reg_alpha': 0, 'reg_lambda': 1, 'min_child_weight': 1}. Best is trial 7 with value: -10.702038764953613.\n",
      "[I 2025-09-15 21:05:17,892] Trial 11 finished with value: -10.828462759653727 and parameters: {'n_estimators': 196, 'max_depth': 5, 'learning_rate': 0.1917409618839949, 'subsample': 0.9392453224933237, 'colsample_bytree': 0.9037200192778952, 'reg_alpha': 0, 'reg_lambda': 1, 'min_child_weight': 3}. Best is trial 7 with value: -10.702038764953613.\n",
      "[I 2025-09-15 21:05:18,393] Trial 12 finished with value: -11.757336616516113 and parameters: {'n_estimators': 176, 'max_depth': 5, 'learning_rate': 0.19736289128221757, 'subsample': 0.938950571776713, 'colsample_bytree': 0.9621517095808241, 'reg_alpha': 0, 'reg_lambda': 1, 'min_child_weight': 2}. Best is trial 7 with value: -10.702038764953613.\n",
      "[I 2025-09-15 21:05:18,808] Trial 13 finished with value: -12.272077242533365 and parameters: {'n_estimators': 123, 'max_depth': 6, 'learning_rate': 0.16692847483130727, 'subsample': 0.8182085542181131, 'colsample_bytree': 0.8905000140203488, 'reg_alpha': 0, 'reg_lambda': 1, 'min_child_weight': 4}. Best is trial 7 with value: -10.702038764953613.\n",
      "[I 2025-09-15 21:05:19,311] Trial 14 finished with value: -10.727384885152182 and parameters: {'n_estimators': 198, 'max_depth': 4, 'learning_rate': 0.13049889310233503, 'subsample': 0.9169623137861925, 'colsample_bytree': 0.8741538035100755, 'reg_alpha': 0, 'reg_lambda': 1, 'min_child_weight': 3}. Best is trial 7 with value: -10.702038764953613.\n",
      "[I 2025-09-15 21:05:19,798] Trial 15 finished with value: -11.257891972859701 and parameters: {'n_estimators': 172, 'max_depth': 5, 'learning_rate': 0.1998699273362871, 'subsample': 0.9636358776312139, 'colsample_bytree': 0.9387040595409455, 'reg_alpha': 0, 'reg_lambda': 1, 'min_child_weight': 4}. Best is trial 7 with value: -10.702038764953613.\n",
      "[I 2025-09-15 21:05:20,114] Trial 16 finished with value: -12.337357680002848 and parameters: {'n_estimators': 125, 'max_depth': 4, 'learning_rate': 0.1414331871033177, 'subsample': 0.7989362906495838, 'colsample_bytree': 0.7820205971305001, 'reg_alpha': 0, 'reg_lambda': 1, 'min_child_weight': 2}. Best is trial 7 with value: -10.702038764953613.\n",
      "[I 2025-09-15 21:05:20,632] Trial 17 finished with value: -11.748575210571289 and parameters: {'n_estimators': 152, 'max_depth': 6, 'learning_rate': 0.17700481680475627, 'subsample': 0.9028301011985785, 'colsample_bytree': 0.8666164726449629, 'reg_alpha': 0, 'reg_lambda': 1, 'min_child_weight': 3}. Best is trial 7 with value: -10.702038764953613.\n",
      "[I 2025-09-15 21:05:20,955] Trial 18 finished with value: -12.025040626525879 and parameters: {'n_estimators': 107, 'max_depth': 5, 'learning_rate': 0.11529649310847112, 'subsample': 0.9618360429964699, 'colsample_bytree': 0.9857508541528562, 'reg_alpha': 0, 'reg_lambda': 1, 'min_child_weight': 2}. Best is trial 7 with value: -10.702038764953613.\n",
      "[I 2025-09-15 21:05:21,361] Trial 19 finished with value: -10.939366658528646 and parameters: {'n_estimators': 168, 'max_depth': 4, 'learning_rate': 0.0723655674941224, 'subsample': 0.8908594004283541, 'colsample_bytree': 0.9289295653117571, 'reg_alpha': 0, 'reg_lambda': 1, 'min_child_weight': 4}. Best is trial 7 with value: -10.702038764953613.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found: {'n_estimators': 137, 'max_depth': 5, 'learning_rate': 0.19753690038993088, 'subsample': 0.9625659107838825, 'colsample_bytree': 0.8934598467451235, 'reg_alpha': 0, 'reg_lambda': 1, 'min_child_weight': 3}\n",
      "Tracked performance for conversation 110d53e6...\n",
      "{'n_estimators': 137, 'max_depth': 5, 'learning_rate': 0.19753690038993088, 'subsample': 0.9625659107838825, 'colsample_bytree': 0.8934598467451235, 'reg_alpha': 0, 'reg_lambda': 1, 'min_child_weight': 3}\n",
      "{'avg_MAE': np.float64(10.702038764953613), 'std_MAE': np.float64(4.53178480287444), 'avg_RMSE': np.float64(17.5551606934487), 'std_RMSE': np.float64(4.643364322744539), 'avg_MAPE': np.float64(0.9703525026481841), 'std_MAPE': np.float64(0.4145505111951835), 'avg_R2': np.float64(0.8755619938354805), 'std_R2': np.float64(0.0738505233524686)}\n",
      "{'lag_features': [1, 2, 7, 14], 'rolling_features': ['mean_7', 'std_14'], 'interaction_features': [], 'time_features': ['month', 'quarter', 'day_of_week'], 'reasoning': 'deterministic-mock'}\n",
      "[{'sender': 'forecaster', 'receiver': 'feature_engineer', 'type': 'performance_feedback', 'timestamp': '2025-09-15T21:05:21.920182'}, {'sender': 'feature_engineer', 'receiver': 'forecaster', 'type': 'improvement_suggestion', 'timestamp': '2025-09-15T21:05:21.920213'}]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from unittest.mock import patch\n",
    "\n",
    "# 1) Build synthetic panel data\n",
    "dates = pd.bdate_range('2022-01-03', periods=40)\n",
    "permnos = [10001, 10002, 10003]\n",
    "rows = []\n",
    "for p in permnos:\n",
    "    prc = np.linspace(10, 12, len(dates)) + np.random.normal(0, 0.2, len(dates))\n",
    "    vol = np.random.randint(1e5, 2e5, len(dates))\n",
    "    ret = np.random.normal(0.0005, 0.01, len(dates))\n",
    "    # Bi-weekly-ish target: step function that changes every 10 biz days\n",
    "    steps = (np.arange(len(dates)) // 10)\n",
    "    short_qty = 1_000 + 50 * steps + np.random.randint(-5, 5, len(dates))\n",
    "    for d, a, b, c, t in zip(dates, prc, vol, ret, short_qty):\n",
    "        rows.append((p, d, a, b, c, t))\n",
    "\n",
    "panel_df = pd.DataFrame(rows, columns=['PERMNO', 'date', 'PRC', 'VOL', 'RET', 'currentShortPositionQuantity'])\n",
    "panel_df = panel_df.set_index(['PERMNO', 'date']).sort_index()\n",
    "\n",
    "# 2) Construct agents/orchestrator (ensure corrected class name usage)\n",
    "feature_engineer = CommunicatingFeatureEngineerLLM(model_name=None)  # LLM will be patched\n",
    "forecaster = CommunicatingForecastingAgent(model_name=None)\n",
    "orchestrator = CommunicatingOrchestrator(feature_engineer, forecaster)\n",
    "\n",
    "# 3) Patch the LLM call to deterministic JSON\n",
    "fake_strategy = \"\"\"\n",
    "{\n",
    "  \"lag_features\": [1, 2, 7, 14],\n",
    "  \"rolling_features\": [\"mean_7\", \"std_14\"],\n",
    "  \"interaction_features\": [],\n",
    "  \"time_features\": [\"month\", \"quarter\", \"day_of_week\"],\n",
    "  \"reasoning\": \"deterministic-mock\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "with patch.object(CommunicatingFeatureEngineerLLM, \"_call_llm\", return_value=fake_strategy):\n",
    "    result = orchestrator.execute_communicating_workflow(\n",
    "        panel_df, \"currentShortPositionQuantity\"\n",
    "    )\n",
    "\n",
    "# 4) Inspect key outputs (asserts in a unit test)\n",
    "print(result[\"forecast_results\"][\"optimization_results\"][\"best_params\"])\n",
    "print(result[\"forecast_results\"][\"cross_validation_metrics\"][\"average_metrics\"])\n",
    "print(result[\"engineered_features\"][\"strategy\"])\n",
    "print(result[\"communication_log\"][:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0dafb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Short-Interest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
